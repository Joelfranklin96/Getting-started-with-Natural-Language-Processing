{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl3PTASoaXSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the necessary packages\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZbknN_h58fB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRsRUdKrwWLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example usage of imported packages"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtwUw4E6bnYi",
        "colab_type": "code",
        "outputId": "a6216d34-085e-4add-f25a-49ae7534d654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Example usage of 'sent_tokenize'\n",
        "\n",
        "a = 'Today is a good day. I would like to do more. Would you like to join me? It is gonna be fun'\n",
        "sent_tokenize(a)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Today is a good day.',\n",
              " 'I would like to do more.',\n",
              " 'Would you like to join me?',\n",
              " 'It is gonna be fun']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znsxrGizUcp7",
        "colab_type": "code",
        "outputId": "5ac5d75a-d8bd-44a7-c82b-a46ea58b0e80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Implementation of 'sent_tokenize' function\n",
        "\n",
        "def sentence(a):\n",
        "  a.replace('?','.').replace('!','.')\n",
        "  return a.split('.')\n",
        "\n",
        "a = 'Today is a good day. I would like to do more. Would you like to join me? It is gonna be fun'\n",
        "\n",
        "sentence(a)\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Today is a good day',\n",
              " ' I would like to do more',\n",
              " ' Would you like to join me? It is gonna be fun']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1wGVCIWj6Bl",
        "colab_type": "code",
        "outputId": "5c3e2f07-b5a5-4737-92a6-11662891d5c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Example usage of 'word_tokenize'\n",
        "\n",
        "a = 'Spending today complaining about. yesterday will not make tomorrow any better'\n",
        "word_tokenize(a)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Spending',\n",
              " 'today',\n",
              " 'complaining',\n",
              " 'about',\n",
              " '.',\n",
              " 'yesterday',\n",
              " 'will',\n",
              " 'not',\n",
              " 'make',\n",
              " 'tomorrow',\n",
              " 'any',\n",
              " 'better']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPqVA-gOj85C",
        "colab_type": "code",
        "outputId": "772c8d78-9087-4bbb-ee6f-71a82a8385b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Implementation of 'word_tokenize' function\n",
        "\n",
        "def word(a):\n",
        "  return a.split(' ')\n",
        "\n",
        "a = 'Spending today complaining about. yesterday will not make tomorrow any better'\n",
        "\n",
        "word(a)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Spending',\n",
              " 'today',\n",
              " 'complaining',\n",
              " 'about.',\n",
              " 'yesterday',\n",
              " 'will',\n",
              " 'not',\n",
              " 'make',\n",
              " 'tomorrow',\n",
              " 'any',\n",
              " 'better']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pruZIfkYtyUi",
        "colab_type": "code",
        "outputId": "9383a346-96fe-4a60-f093-0d0779a41e59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(set(stopwords.words('english')))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'re', \"mightn't\", 's', 'haven', 'had', 'more', \"don't\", 'theirs', 'his', 'during', \"weren't\", 'ain', \"isn't\", 'because', 'ma', \"should've\", 'am', \"wasn't\", 'that', 'hadn', 'they', \"wouldn't\", 'while', 'our', 'itself', \"shan't\", 'an', 'on', 'these', 'again', 'same', 'just', 'them', 'does', 'above', \"hasn't\", 'after', 'herself', 'is', 'myself', 'y', 'for', 'm', 'should', 'or', 'between', 'd', 'won', 'with', 'such', 'ourselves', 'this', 'in', 'hasn', 'most', 'shan', \"mustn't\", \"hadn't\", 'to', 'when', 'off', \"you'll\", 'by', 'll', 'mightn', 'couldn', 'don', 'before', 'the', 'then', 'out', \"shouldn't\", 'as', 'will', 'aren', 'down', 'needn', 'yourselves', 'own', 'its', \"won't\", 'doesn', 'about', 'up', 'at', 'she', 'were', 'if', 'few', 'their', 'any', 'yourself', 'i', 'under', 'me', 'wasn', \"doesn't\", 'further', 'here', 'your', 'what', 'mustn', 'ours', 'do', 'and', 'only', 'o', \"that'll\", \"you've\", 'yours', 'be', 'over', 'being', 'no', 'until', 'below', 'into', 'are', 'a', 'which', 'of', 've', 'whom', \"she's\", 'both', 'too', 'you', 'nor', 'themselves', 'than', 'can', 'now', 't', 'doing', 'how', \"you're\", 'have', 'been', 'having', 'from', 'who', 'once', \"haven't\", 'so', 'her', 'some', 'it', 'weren', 'all', \"couldn't\", 'was', 'but', 'very', \"aren't\", 'himself', 'did', 'he', 'not', 'didn', 'why', 'isn', 'against', \"it's\", 'each', 'hers', \"needn't\", 'my', 'through', \"you'd\", 'him', \"didn't\", 'where', 'wouldn', 'has', 'we', 'those', 'shouldn', 'other', 'there'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f73Z0KfUu2ay",
        "colab_type": "code",
        "outputId": "00b2c5ee-47f3-4ab3-a42d-c067ea6c8187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Example usage of 'PorterStemmer'\n",
        "\n",
        "a = ['lift','lifts','lifting','lifter','lifted']\n",
        "a_stemmed = []\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "for x in a:\n",
        "  a_stemmed.append(ps.stem(x))\n",
        "\n",
        "print(a_stemmed)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['lift', 'lift', 'lift', 'lifter', 'lift']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgzk39Ytcz4v",
        "colab_type": "code",
        "outputId": "7d8cdc3f-106a-4dda-84fb-01b4649d1b57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Example usage of PunktSentenceTokenizer\n",
        "\n",
        "from nltk import PunktSentenceTokenizer\n",
        "\n",
        "train_txt = 'Mr. Mike met Shane. The appointment was critical. Mr. Mike was in serious condition'\n",
        "sample_txt = 'Mr. Watson met Mark. The appointment was critical. Mr. Watson was in serious condition'\n",
        "\n",
        "custom_tokenizer = PunktSentenceTokenizer(train_txt)\n",
        "tokenized = custom_tokenizer.tokenize(sample_txt)\n",
        "tokenized\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mr. Watson met Mark.',\n",
              " 'The appointment was critical.',\n",
              " 'Mr. Watson was in serious condition']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsXTFqS8fxYm",
        "colab_type": "code",
        "outputId": "6537e9c7-1c96-42ba-9b3f-3df5a1044066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# Example usage of POS Tagger\n",
        "\n",
        "sample_txt = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
        "word = nltk.word_tokenize(sample_txt)\n",
        "tag = nltk.pos_tag(word)\n",
        "tag"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('European', 'JJ'),\n",
              " ('authorities', 'NNS'),\n",
              " ('fined', 'VBD'),\n",
              " ('Google', 'NNP'),\n",
              " ('a', 'DT'),\n",
              " ('record', 'NN'),\n",
              " ('$', '$'),\n",
              " ('5.1', 'CD'),\n",
              " ('billion', 'CD'),\n",
              " ('on', 'IN'),\n",
              " ('Wednesday', 'NNP'),\n",
              " ('for', 'IN'),\n",
              " ('abusing', 'VBG'),\n",
              " ('its', 'PRP$'),\n",
              " ('power', 'NN'),\n",
              " ('in', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('mobile', 'JJ'),\n",
              " ('phone', 'NN'),\n",
              " ('market', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('ordered', 'VBD'),\n",
              " ('the', 'DT'),\n",
              " ('company', 'NN'),\n",
              " ('to', 'TO'),\n",
              " ('alter', 'VB'),\n",
              " ('its', 'PRP$'),\n",
              " ('practices', 'NNS')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXocqY2aEe1u",
        "colab_type": "code",
        "outputId": "07e53b55-defe-41d9-fd0b-4877712dd4eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nltk.help.upenn_tagset()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "$: dollar\n",
            "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
            "'': closing quotation mark\n",
            "    ' ''\n",
            "(: opening parenthesis\n",
            "    ( [ {\n",
            "): closing parenthesis\n",
            "    ) ] }\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ! ?\n",
            ":: colon or ellipsis\n",
            "    : ; ...\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "CD: numeral, cardinal\n",
            "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
            "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
            "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n",
            "EX: existential there\n",
            "    there\n",
            "FW: foreign word\n",
            "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
            "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
            "    terram fiche oui corporis ...\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "JJR: adjective, comparative\n",
            "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
            "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
            "    cozier creamier crunchier cuter ...\n",
            "JJS: adjective, superlative\n",
            "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
            "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
            "    dearest deepest densest dinkiest ...\n",
            "LS: list item marker\n",
            "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
            "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
            "    two\n",
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "NNPS: noun, proper, plural\n",
            "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
            "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
            "    Apache Apaches Apocrypha ...\n",
            "NNS: noun, common, plural\n",
            "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
            "    divestitures storehouses designs clubs fragrances averages\n",
            "    subjectivists apprehensions muses factory-jobs ...\n",
            "PDT: pre-determiner\n",
            "    all both half many quite such sure this\n",
            "POS: genitive marker\n",
            "    ' 's\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "RBR: adverb, comparative\n",
            "    further gloomier grander graver greater grimmer harder harsher\n",
            "    healthier heavier higher however larger later leaner lengthier less-\n",
            "    perfectly lesser lonelier longer louder lower more ...\n",
            "RBS: adverb, superlative\n",
            "    best biggest bluntest earliest farthest first furthest hardest\n",
            "    heartiest highest largest least less most nearest second tightest worst\n",
            "RP: particle\n",
            "    aboard about across along apart around aside at away back before behind\n",
            "    by crop down ever fast for forth from go high i.e. in into just later\n",
            "    low more off on open out over per pie raising start teeth that through\n",
            "    under unto up up-pp upon whole with you\n",
            "SYM: symbol\n",
            "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "    to\n",
            "UH: interjection\n",
            "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
            "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
            "    man baby diddle hush sonuvabitch ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n",
            "VBD: verb, past tense\n",
            "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
            "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
            "    speculated wore appreciated contemplated ...\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
            "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
            "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
            "WDT: WH-determiner\n",
            "    that what whatever which whichever\n",
            "WP: WH-pronoun\n",
            "    that what whatever whatsoever which who whom whosoever\n",
            "WP$: WH-pronoun, possessive\n",
            "    whose\n",
            "WRB: Wh-adverb\n",
            "    how however whence whenever where whereby whereever wherein whereof why\n",
            "``: opening quotation mark\n",
            "    ` ``\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddswci5My4Ua",
        "colab_type": "code",
        "outputId": "305d3a9c-ddf1-4c23-d664-38fedaf25cad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "# Example usage of Named Entity Recognition\n",
        "\n",
        "sample_txt = 'Amit Shah asserted the government will ensure that the non-Muslim refugees from Pakistan, Bangladesh and Afghanistan get Indian nationality and live in the country with honour.'\n",
        "words = nltk.word_tokenize(sample_txt)\n",
        "tagged = nltk.pos_tag(words)\n",
        "named_entity = nltk.ne_chunk(tagged,binary = False)\n",
        "\n",
        "print(named_entity)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (PERSON Amit/NNP)\n",
            "  (PERSON Shah/NNP)\n",
            "  asserted/VBD\n",
            "  the/DT\n",
            "  government/NN\n",
            "  will/MD\n",
            "  ensure/VB\n",
            "  that/IN\n",
            "  the/DT\n",
            "  non-Muslim/JJ\n",
            "  refugees/NNS\n",
            "  from/IN\n",
            "  (GPE Pakistan/NNP)\n",
            "  ,/,\n",
            "  (ORGANIZATION Bangladesh/NNP)\n",
            "  and/CC\n",
            "  (GPE Afghanistan/NNP)\n",
            "  get/VB\n",
            "  (GPE Indian/JJ)\n",
            "  nationality/NN\n",
            "  and/CC\n",
            "  live/NN\n",
            "  in/IN\n",
            "  the/DT\n",
            "  country/NN\n",
            "  with/IN\n",
            "  honour/NN\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sV6XS9yaNlF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "971d2c81-63c9-4f59-ecdf-bbc2380a5cde"
      },
      "source": [
        "# Example usage of Chunking\n",
        "\n",
        "from nltk.corpus import state_union\n",
        "\n",
        "# Defining the training and sample text\n",
        "\n",
        "train_text = state_union.raw('2005-GWBush.txt')\n",
        "sample_txt = state_union.raw('2006-GWBush.txt')\n",
        "\n",
        "# Training the tokenizer and tokenizing the sample text\n",
        "\n",
        "custom_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "tokenized = custom_tokenizer.tokenize(sample_txt)\n",
        "\n",
        "# Defining function which chunks a noun phrase\n",
        "\n",
        "def process_content():\n",
        "  for i in tokenized[7:9]:\n",
        "    words = nltk.word_tokenize(i)\n",
        "    tagged = nltk.pos_tag(words)\n",
        "\n",
        "    chunkgram = r'''chunk: {<RB.?>*<VB.?>*<NNP>*<NN>?}'''\n",
        "    chunkparser = nltk.RegexpParser(chunkgram)\n",
        "    chunked = chunkparser.parse(tagged)\n",
        "\n",
        "    print(chunked)\n",
        "\n",
        "process_content()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  We/PRP\n",
            "  (chunk have/VBP gathered/VBN)\n",
            "  under/IN\n",
            "  this/DT\n",
            "  (chunk Capitol/NNP dome/NN)\n",
            "  in/IN\n",
            "  moments/NNS\n",
            "  of/IN\n",
            "  national/JJ\n",
            "  (chunk mourning/NN)\n",
            "  and/CC\n",
            "  national/JJ\n",
            "  (chunk achievement/NN)\n",
            "  ./.)\n",
            "(S\n",
            "  We/PRP\n",
            "  (chunk have/VBP served/VBN America/NNP)\n",
            "  through/IN\n",
            "  one/CD\n",
            "  of/IN\n",
            "  the/DT\n",
            "  (chunk most/RBS)\n",
            "  consequential/JJ\n",
            "  periods/NNS\n",
            "  of/IN\n",
            "  our/PRP$\n",
            "  (chunk history/NN)\n",
            "  --/:\n",
            "  and/CC\n",
            "  it/PRP\n",
            "  (chunk has/VBZ been/VBN)\n",
            "  my/PRP$\n",
            "  (chunk honor/NN)\n",
            "  to/TO\n",
            "  (chunk serve/VB)\n",
            "  with/IN\n",
            "  you/PRP\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HYgootMXkbv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f11490d-9c14-414a-a8a3-1cf02b1e9113"
      },
      "source": [
        "# Example usage of Chinking\n",
        "\n",
        "# Defining the training and sample text\n",
        "\n",
        "train_text = state_union.raw('2005-GWBush.txt')\n",
        "sample_txt = state_union.raw('2006-GWBush.txt')\n",
        "\n",
        "# Training the tokenizer and tokenizing the sample text\n",
        "\n",
        "custom_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "tokenized = custom_tokenizer.tokenize(sample_txt)\n",
        "\n",
        "# Defining function which chunks a noun phrase\n",
        "\n",
        "def process_content():\n",
        "  for i in tokenized[0:3]:\n",
        "    words = nltk.word_tokenize(i)\n",
        "    tagged = nltk.pos_tag(words)\n",
        "\n",
        "# The main difference here is the }{ vs the {}. }{ means we are removing\n",
        "# from the chink one or more verbs, prepositions, determiners, or the word 'to'.\n",
        "\n",
        "    chunkgram = r'''chunk: {<.*>+} \n",
        "    }<VB.?|IN|DT|TO>+{'''\n",
        "    chunkparser = nltk.RegexpParser(chunkgram)\n",
        "    chunked = chunkparser.parse(tagged)\n",
        "\n",
        "    print(chunked)\n",
        "\n",
        "process_content()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP 'S/POS ADDRESS/NNP)\n",
            "  BEFORE/IN\n",
            "  (chunk A/NNP JOINT/NNP SESSION/NNP)\n",
            "  OF/IN\n",
            "  (chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
            "  OF/IN\n",
            "  (chunk\n",
            "    THE/NNP\n",
            "    UNION/NNP\n",
            "    January/NNP\n",
            "    31/CD\n",
            "    ,/,\n",
            "    2006/CD\n",
            "    THE/NNP\n",
            "    PRESIDENT/NNP\n",
            "    :/:\n",
            "    Thank/NNP\n",
            "    you/PRP)\n",
            "  all/DT\n",
            "  (chunk ./.))\n",
            "(S\n",
            "  (chunk\n",
            "    Mr./NNP\n",
            "    Speaker/NNP\n",
            "    ,/,\n",
            "    Vice/NNP\n",
            "    President/NNP\n",
            "    Cheney/NNP\n",
            "    ,/,\n",
            "    members/NNS)\n",
            "  of/IN\n",
            "  (chunk Congress/NNP ,/, members/NNS)\n",
            "  of/IN\n",
            "  the/DT\n",
            "  (chunk\n",
            "    Supreme/NNP\n",
            "    Court/NNP\n",
            "    and/CC\n",
            "    diplomatic/JJ\n",
            "    corps/NN\n",
            "    ,/,\n",
            "    distinguished/JJ\n",
            "    guests/NNS\n",
            "    ,/,\n",
            "    and/CC\n",
            "    fellow/JJ\n",
            "    citizens/NNS\n",
            "    :/:)\n",
            "  Today/VB\n",
            "  (chunk our/PRP$ nation/NN)\n",
            "  lost/VBD\n",
            "  a/DT\n",
            "  beloved/VBN\n",
            "  (chunk ,/, graceful/JJ ,/, courageous/JJ woman/NN who/WP)\n",
            "  called/VBD\n",
            "  (chunk America/NNP)\n",
            "  to/TO\n",
            "  (chunk its/PRP$ founding/NN ideals/NNS and/CC)\n",
            "  carried/VBD\n",
            "  on/IN\n",
            "  a/DT\n",
            "  (chunk noble/JJ dream/NN ./.))\n",
            "(S\n",
            "  (chunk Tonight/NN we/PRP)\n",
            "  are/VBP\n",
            "  comforted/VBN\n",
            "  by/IN\n",
            "  the/DT\n",
            "  (chunk hope/NN)\n",
            "  of/IN\n",
            "  a/DT\n",
            "  (chunk glad/JJ reunion/NN)\n",
            "  with/IN\n",
            "  the/DT\n",
            "  (chunk husband/NN who/WP)\n",
            "  was/VBD\n",
            "  taken/VBN\n",
            "  (chunk so/RB long/RB ago/RB ,/, and/CC we/PRP)\n",
            "  are/VBP\n",
            "  (chunk grateful/JJ)\n",
            "  for/IN\n",
            "  the/DT\n",
            "  (chunk good/JJ life/NN)\n",
            "  of/IN\n",
            "  (chunk Coretta/NNP Scott/NNP King/NNP ./.))\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
